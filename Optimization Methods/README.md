# Optimization Methods
---
## Table Content
* [Introduction](#Introduction)
* [Algorithms](#Algorithms)
    - [Gradient Descent](#Gradient)
* [Challenges](#Challenges)
* [Reference](#Reference)

---
## Introduction  <a class="anchor" id="Introduction"></a>
**Optimization** is a group of methods used for finding a global or local minimum of a cost function. It's important in the field of machine learning because it provides an iteractive way to update weights for most all models.

---
## Algorithms <a class="anchor" id="Algorithms"></a>

### Gradient Descent <a class="anchor" id="Gradient"></a>
Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. Stochastic gradient descent and mini-batch gradient descent are two variants of (batch) gradient descent.

---
## Challenges <a class="anchor" id="Challenges"></a>

These problems include taking too long to train, vanishing and exploding gradients and initialization. 
* Taking lots of time in training process.
* Vanishing and exploding gradients.
* Cannot guarantee global minimum, but usually local minimum.
* Result is highly dependent on initialization.

---
## Reference <a class="anchor" id="Reference"></a>
1.Wikimedia Foundation. (2021, November 2). Gradient descent. Wikipedia. Retrieved December 13, 2021, from https://en.wikipedia.org/wiki/Gradient_descent. 
