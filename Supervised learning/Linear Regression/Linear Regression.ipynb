{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Linear Regreesion (LR) Algorithm and Applications\n",
    "#### Language: Python 3.8.8\n",
    "#### Author: Tianjian Sun\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "* [Algorithm](#Algorithm)\n",
    "* [Illustration](#Illustration)\n",
    "* [Advantages and Disadvantages](#Advantages_and_Disadvantages)\n",
    "    * [Advantages](#Advantages)\n",
    "    * [Disadvantages](#Disadvantages)\n",
    "* [Hard code of KNN classifier and regressor](#Code)\n",
    "* [Applications on data sets](#Applications)\n",
    "    * [Classification problem](#Classification)\n",
    "    * [Regression problem](#Regression)\n",
    "* [How can $k$ impact prediction result](#k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Introduction <a class=\"anchor\" id=\"Introduction\"></a>\n",
    "In this section we focus on a straight-forward and classic statistical model, linear regreesion (LR). The main idea of linear regression is to use a linear predictor function whose unknown model parameters are estimated from the data. \n",
    "\n",
    "(Least squares) linear regreesion is a classic and old supervised model, back to 1800's, finding a good rough linear fit to a set of points was performed by [Legendre](https://en.wikipedia.org/wiki/Adrien-Marie_Legendre) (1805) and [Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss) (1809) for the prediction of planetary movement. \n",
    "\n",
    "Linear regreesion is widely used in most regression tasks, variance analysis, and dependence analysis, and it plays an important role in the subfield of machine learning. The linear regression algorithm is one of the fundamental supervised machine-learning algorithms due to its relative simplicity and well-known properties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Algorithm <a class=\"anchor\" id=\"Algorithm\"></a>\n",
    "\n",
    "Assume we have an input data set (column vector) $\\textbf{X} = (\\textbf{x}_1^T, \\textbf{x}_2^T, \\cdots, \\textbf{x}_n^T)$, where $\\textbf{x}_i^T = (x_{i1}, x_{i2}, \\cdots, x_{ip}) \\in \\R^p$, and want to predict a real-valued output data set (column vector) $\\textbf{y} = (y_1, y_2, \\cdots, y_n)$ . The linear regression model has the form\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\sum_{j=1}^{p} {\\beta_j x_{ij}} + \\epsilon_i \n",
    "$$\n",
    "\n",
    "Here the $\\beta_j’s$ are unknown parameters or coefficients of corresponding input.\n",
    "\n",
    "To simplify the equation, usually a column of $1$'s are added to the input data matrix, where $\\textbf{x}_i^T = (1, x_{i1}, x_{i2}, \\cdots, x_{ip})$, and the linear equation becomes\n",
    "\n",
    "$$\n",
    "y_i = \\textbf{x}_i^T \\bm{\\beta} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "Often these n equations are stacked together and written in a matrix form\n",
    "\n",
    "$$\n",
    "\\textbf{y} = \\textbf{X} \\bm{\\beta} + \\epsilon\n",
    "$$\n",
    "\n",
    "The linear model either assumes that the regression function $E(Y|X)$ is linear, or that the linear model is a reasonable approximation. \n",
    "\n",
    "The most popular estimation method is least squares, in which we pick the coefficients $\\bm{\\beta} = (\\beta_0, \\beta_1, . . ., \\beta_p)^T$, which minimizes the residual sum of squares\n",
    "\n",
    "$$\n",
    "RSS(\\bm{\\beta}) = \\sum_{i=1}^n {\\epsilon_i^2} =\\sum_{i=1}^n (y_i - \\beta_0 - \\sum_{j=1}^{p} {\\beta_j x_{ij}})^2\n",
    "$$\n",
    "\n",
    "This criterion makes sense if the observations $(\\bm{x}_i, y_i)$ are independent and identically distributed (iid), which means that they are and randomly drawn from the population.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize $RSS(\\bm{\\beta})$, first we make the following assumptions:\n",
    "* $E[\\epsilon_i]=0$, i.e., $E[y_i]=\\textbf{x}_i^T \\bm{\\beta}$\n",
    "* $Var[\\epsilon_i] = \\sigma^2 < \\infty$ (homoscedasticity)\n",
    "* $Cov[\\epsilon_i, \\epsilon_j]=0$ for $\\forall i \\neq j$, i.e., $\\epsilon_i, \\epsilon_j$ are uncorrelated,\n",
    "and $\\epsilon_i$ do not depend on $x_i$ .\n",
    "\n",
    "\n",
    "\n",
    "Rewrite it to matrix form\n",
    "$$\n",
    "RSS(\\bm{\\beta}) = (\\textbf{y}-\\textbf{X}\\bm{\\beta})^T (\\textbf{y}-\\textbf{X}\\bm{\\beta})\n",
    "$$\n",
    "\n",
    "Notice that it's a quadratic function with $p+1$ paraeters, take the partial derivative w.r.t $\\bm{\\beta}$, we have\n",
    "\n",
    "$$\n",
    "\\frac{\\partial RSS} {\\partial \\bm{\\beta}} = -2 \\textbf{X}^T (\\textbf{y}-\\textbf{X}\\bm{\\beta})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial^2 RSS} {\\partial \\bm{\\beta} \\partial \\bm{\\beta}^T} = -2 \\textbf{X}^T \\textbf{X}\n",
    "$$\n",
    "\n",
    "Assume that $\\textbf{X}$ is fully ranked, i.e., $n>p$, then $\\textbf{X}^T \\textbf{X}$ is positive definite. Set the first partial derivative to $0$, we have\n",
    "$$\n",
    "\\textbf{X}^T (\\textbf{y}-\\textbf{X}\\bm{\\beta}) = \\textbf{0}\n",
    "$$\n",
    "\n",
    "And the solution is \n",
    "$$\n",
    "\\hat{\\bm{\\beta}} = (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}\n",
    "$$\n",
    "\n",
    "Note that only when $\\textbf{X}$ is fully ranked does the inverse of $\\textbf{X}^T \\textbf{X}$ exist.\n",
    "\n",
    "Thus the fitted values of the training inputs are given by\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{y}} = \\textbf{X} \\hat{\\bm{\\beta}} = \\textbf{X} (\\textbf{X}^T \\textbf{X})^{-1} \\textbf{X}^T \\textbf{y}\n",
    "$$\n",
    "\n",
    "To make a prediction given an unobserved input vector $\\textbf{x}_0$\n",
    "\n",
    "$$\n",
    "\\hat{y} = (1, \\textbf{x}_0)^T \\hat{\\bm{\\beta}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Illustration <a class=\"anchor\" id=\"Illustration\"></a>\n",
    "\n",
    "Take a look at the two-dimension space, by Sewaqu ([link](https://commons.wikimedia.org/w/index.php?curid=11967659)), we can have a intuitive idea about how linear regression works.\n",
    "\n",
    "Let the input data $\\textbf{X}$ be in one-dimensional space, i.e., $\\textbf{X} = (x_1, x_2, \\cdots, x_n)$, and corresponding output be $\\textbf{y}=(y_1, y_2, \\cdots, y_n)$. The $y-x$ scatter plot is below, and the red line represents the OLS regression line. It's intuitive that the linear regression line \"goes cross\" all data points, and shows the linear trend of how output goes as input changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lr_illu](images/Linear_regression.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Advantages and Disadvantages <a class=\"anchor\" id=\"Advantages_and_Disadvantages\"></a>\n",
    "\n",
    "#### Advantages: <a class=\"anchor\" id=\"Advantages\"></a>\n",
    "\n",
    "* Best Linear Unbiased Estimator. The least square estimator of linear regression model has great properties. By the [Gauss-Markov theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem), the ordinary least squares (OLS) regression produces unbiased estimates that have the smallest variance of all possible linear estimators. This property is called BLUE (Best Linear Unbiased Estimator).\n",
    "\n",
    "* Simple model. The Linear regression model is the simplest equation using which the relationship between the multiple predictor variables and predicted variable can be expressed, and the ordinary least squares error function is also very simple and straight-forward.\n",
    "\n",
    "* Computationally friendly. The modeling speed of linear regression is fast as it does not require complicated calculations and runs predictions fast when the amount of data is large.\n",
    "\n",
    "* Great interpretability of the output. The ability of linear regression to determine the relative influence of one or more predictor variables to the predicted value when the predictors are independent of each other is one of the key reasons of the popularity of linear regression. The model derived using this method can express the what change in the predictor variable causes what change in the predicted or target variable.\n",
    "\n",
    "\n",
    "#### Disadvantages: <a class=\"anchor\" id=\"Disadvantages\"></a>\n",
    "\n",
    "* Overly-Simplistic. The linear regression model is too simplistic to capture real world complexity. The linear regression assumes a linear relationship between independent variables and dependent variable, which cannot represent more complex relationships in real world. \n",
    "\n",
    "* Independence of variables. Assumes that the predictor variables are not correlated which is rarely true. It is important to, therefore, remove multicollinearity (using dimensionality reduction techniques) because the technique assumes that there is no relationship among independent variables. In cases of high multicollinearity, two features that have high correlation will influence each other’s weight and result in an unreliable model.\n",
    "\n",
    "* Homoscedasticity. The linear regression model assumes that independent variables can represent all expectation and variance of dependent variable (so that $E[\\epsilon_i]=0$ and $Var[\\epsilon_i] = \\sigma^2$), which is always not true because in real world it's hard for people to find exactly all independent variables that affect the dependent variable. Usually people will have input that are dependent and insufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Hard code of linear regression model <a class=\"anchor\" id=\"Code\"></a>\n",
    "\n",
    "#### import necessary packages\n",
    "* [numpy](https://numpy.org/)\n",
    "* [pandas](https://pandas.pydata.org/)\n",
    "* [sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html)\n",
    "* [matplotlib](https://matplotlib.org/)\n",
    "* [collections](https://docs.python.org/3/library/collections.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader as web\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function of linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression():\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.bias = None\n",
    "        self.beta_hat = None\n",
    "\n",
    "    def fit(self, X, y, bias=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.bias = bias\n",
    "        if bias:\n",
    "            ones_column = np.ones(X.shape[0])\n",
    "            X.append(ones_column)\n",
    "\n",
    "        beta_hat = np.linalg.inv(X.T @ X) @X.T @ y\n",
    "        self.beta_hat = beta_hat\n",
    "\n",
    "    def predict(self, x):\n",
    "        return x @ self.beta_hat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use root mean square error (RMSE) and coefficient of determination ($R^2$) to estimate our linear regression model. $R^2$ is the proportion of the variation in the dependent variable that is predictable from the independent variable(s). It's defined as follows\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{res}} {SS_{tot}}\n",
    "$$\n",
    "where $SS_{res}$ is the sum of squares of residuals\n",
    "$$\n",
    "SS_{res} = \\sum_{i=1}^n {(y_i-\\hat{y}_i)^2}\n",
    "$$\n",
    "and $SS_{tot}$ is the total sum of squares\n",
    "$$\n",
    "SS_{tot} = \\sum_{i=1}^n {(y_i-\\bar{y}_i)^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(true, pred):\n",
    "    return np.sqrt((true-pred).T@(true-pred))\n",
    "\n",
    "\n",
    "def R_2(true, pred):\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e918aaa81d99c652401bdd1a0c185581595fb477ac919641bd65261b5d7782a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
