{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Multilayer Neural Network and Stochastic/Mini Batch Gradient Descent\n",
    "#### Language: Python 3.8.8\n",
    "#### Author: Tianjian Sun\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "- [Introduction](#Introduction)\n",
    "- [Algorithm](#Algorithm)\n",
    "    - [Multilayer Neural Network](#Multilayer)\n",
    "    - [Batch Gradient Descent](#Batch)\n",
    "    - [Stochastic Gradient Descent](#Stochastic)\n",
    "    - [Mini Batch Gradient Descent](#Mini)\n",
    "- [Illustration](#Illustration)\n",
    "- [Advantages and Disadvantages of Multilayer Neural Network](#Advantages)\n",
    "    - [Advantages](#Advantages)\n",
    "    - [Disadvantages](#Disadvantages)\n",
    "- [Comparison of Gradient Descent Methods](#Comparison)\n",
    "- [Code of Multilayer Neural Network and Gradient Descent](#Code)\n",
    "- [Applications on data sets](#Applications)\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction <a class=\"anchor\" id=\"Introduction\"></a>\n",
    "\n",
    "A [multilayer neural network](https://en.wikipedia.org/wiki/Multilayer_perceptron) (or multilayer perceptron ,MLP) is a class of feedforward artificial neural network (ANN). It's an extension of perceptrons, with more layers and non-linear activation functions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm <a class=\"anchor\" id=\"Algorithm\"></a>\n",
    "\n",
    "#### Multilayer Neural Network <a class=\"anchor\" id=\"Multilayer\"></a>\n",
    "\n",
    "First, a multilayer neural network has a non-linear activation function in most cases, and *sigmoid* ($\\sigma(\\cdot)$), *hyperbolic tangent* ($tanh(\\cdot)$) and *rectified linear* ($RELU(\\cdot)$) activation functions are widely used.\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "tanh(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "RELU(x) = \\begin{cases} x & \\text{if}\\ x > 0,\\\\0 & \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "\n",
    "Further, a MLP consists of many layers (an input and an output layer with one or more hidden layers) of non-linear activating nodes. Since MLPs are fully connected, each node in one layer connects with a certain weight to every node in the following layer.\n",
    "\n",
    "Learning occurs in MLP by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is a supervised learning, and is carried out through [backpropagation](https://en.wikipedia.org/wiki/Backpropagation), a generalization of the least mean squares algorithm in the linear perceptron.\n",
    "\n",
    "The basic idea of backpropagation is to take advantage of the [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule) to derive derivative of each layers, from end to start.\n",
    "\n",
    "Error in an output node $j$ in the $n$ th training example by $e_j(n)=d_j(n)-y_j(n)$, where $d$ is the target value and $y$ is the value produced by the perceptron. The node weights can be adjusted by minimizing the sum square error in the entire output, given by\n",
    "\n",
    "$$\\mathcal{E}(n)=\\frac{1}{2}\\sum_j e_j^2(n)$$\n",
    "\n",
    "Using gradient descent, the change in each weight of output layer is\n",
    "\n",
    "$$\\Delta w_{ji} (n) = -\\gamma\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} y_i(n)$$\n",
    "\n",
    "where $y_i$ is the output of the previous neuron, $\\gamma$ is the *learning rate*, $v_j$ is the output of output layer.\n",
    "\n",
    "It is easy to prove that for an output node this derivative can be simplified to\n",
    "\n",
    "$$-\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} = e_j(n)\\phi^\\prime (v_j(n))$$\n",
    "\n",
    "where $\\phi^\\prime$ is the derivative of the activation function described above, which itself does not vary. For hidden nodes, it can be shown that the relevant derivative is\n",
    "\n",
    "$$-\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} = \\phi^\\prime (v_j(n))\\sum_k -\\frac{\\partial\\mathcal{E}(n)}{\\partial v_k(n)} w_{kj}(n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent <a class=\"anchor\" id=\"Batch\"></a>\n",
    "\n",
    "Batch gradient descent just take all the training data in every step of gradient descent, so it's just the basic form of gradient descent descussed in the **Gradient \n",
    "Descent** ssection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent <a class=\"anchor\" id=\"Stochastic\"></a>\n",
    "\n",
    "Stochastic Gradient Descent (SGD) considers one random sample in every step. As only one sample is used, the loss will not necessarily decrease. But the loss will decrease in a long training period. SGD generally converges faster when the dataset is large as it causes updates to the parameters more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Batch Gradient Descent <a class=\"anchor\" id=\"Mini\"></a>\n",
    "\n",
    "Mini Batch Gradient Descent considers a fixed size of random samples in every step. It's a trde-off of Batch and Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Illustration <a class=\"anchor\" id=\"Illustration\"></a>\n",
    "\n",
    "**Multilayer Neural Network**\n",
    "\n",
    "A multilayer neural network is multilayer of perceptrons. It consists an input layer, several hidden layers, output layer, fully connected weights and non-linear activation function. The following image is is from [Wikipedia](https://commons.wikimedia.org/wiki/File:MultiLayerPerceptron.png).\n",
    "\n",
    "<img src=\"images/Multilayer-Perceptron-Network.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Advantages and Disadvantages of Multilayer Neural Network <a class=\"anchor\" id=\"Advantages\"></a>\n",
    "\n",
    "#### Advantage of Multilayer Neural Network <a class=\"anchor\" id=\"Advantages\"></a>\n",
    "- Learn non-linear and complex relationships.\n",
    "- Fit all continous function, if the hidden layer is more than 2.\n",
    "- Can work on both classification and regression\n",
    "#### Disadvantage of Perceptron <a class=\"anchor\" id=\"Disadvantage\"></a>\n",
    "- Too many parameters to turn\n",
    "- Large model complexity if number of layers and neurons are large\n",
    "- Slow training process if model is large\n",
    "- Easy to overfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Comparison of Gradient Descent Methods <a class=\"anchor\" id=\"Comparison\"></a>\n",
    "\n",
    "#### Advantages of Batch Gradient Descent\n",
    "- Can fix learning rate during training.\n",
    "- Guaranteed convergency.\n",
    "- Unbiased estimator of gradients.\n",
    "\n",
    "#### Disadvantages of Batch Gradient Descent\n",
    "- Slow on large data set.\n",
    "- Waste time on some data which have little attribution to weights update.\n",
    "\n",
    "#### Advantages of Stochastic Gradient Descent\n",
    "- Fastest to run\n",
    "- Avoid redundant samples\n",
    "- Most noise added\n",
    "\n",
    "#### Disadvantages of Stochastic Gradient Descent\n",
    "- No guaranteed convergency.\n",
    "- More oscillations in training\n",
    "\n",
    "#### Advantages of Mini Batch Gradient Descent\n",
    "- Faster to run\n",
    "- Avoid redundant samples\n",
    "\n",
    "#### Disadvantages of Mini Batch Gradient Descent\n",
    "- No guaranteed convergency.\n",
    "- More oscillations in training\n",
    "- More noise added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Code of MLP <a class=\"anchor\" id=\"Code\"></a>\n",
    "\n",
    "#### import necessary packages\n",
    "* [numpy](https://numpy.org/)\n",
    "* [pandas](https://pandas.pydata.org/)\n",
    "* [sklearn](https://scikit-learn.org/stable/datasets/toy_dataset.html)\n",
    "* [keras](https://keras.io/)\n",
    "* [matplotlib](https://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron():\n",
    "  \n",
    "    def __init__(self, layers = [784, 60, 60, 10], actFun_type='relu'):\n",
    "        self.actFun_type = actFun_type\n",
    "        self.layers = layers\n",
    "        self.L = len(self.layers)\n",
    "        self.W =[[0.0]]\n",
    "        self.B = [[0.0]]\n",
    "        for i in range(1, self.L):\n",
    "            w_temp = np.random.randn(self.layers[i], self.layers[i-1]) * np.sqrt(2/self.layers[i-1])\n",
    "            b_temp = np.random.randn(self.layers[i], 1) * np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "            self.W.append(w_temp)\n",
    "            self.B.append(b_temp)\n",
    "\n",
    "    def reset_weights(self, layers = [784, 60, 60, 10]):\n",
    "        self.layers = layers\n",
    "        self.L = len(self.layers)\n",
    "        self.W = [[0.0]]\n",
    "        self.B = [[0.0]]\n",
    "        for i in range(1, self.L):\n",
    "            w_temp = np.random.randn(self.layers[i], self.layers[i-1])*np.sqrt(2/self.layers[i-1])\n",
    "            b_temp = np.random.randn(self.layers[i], 1)*np.sqrt(2/self.layers[i-1])\n",
    "\n",
    "            self.W.append(w_temp)\n",
    "            self.B.append(b_temp)\n",
    "\n",
    "    def forward_pass(self, p, predict_vector = False):\n",
    "        Z =[[0.0]]\n",
    "        A = [p[0]]\n",
    "        for i in range(1, self.L):\n",
    "            z = (self.W[i] @ A[i-1]) + self.B[i]\n",
    "            a = self.actFun(z, self.actFun_type)\n",
    "            Z.append(z)\n",
    "            A.append(a)\n",
    "\n",
    "        if predict_vector == True:\n",
    "            return A[-1]\n",
    "        else:\n",
    "            return Z, A\n",
    "\n",
    "    def mse(self, a, y):\n",
    "        return .5*sum((a[i]-y[i])**2 for i in range(10))[0]\n",
    "\n",
    "    def MSE(self, data):\n",
    "        c = 0.0\n",
    "        for p in data:\n",
    "            a = self.forward_pass(p, predict_vector=True)\n",
    "            c += self.mse(a, p[1])\n",
    "        return c/len(data)\n",
    "\n",
    "    def actFun(self, z, type):\n",
    "        if type == 'tanh':\n",
    "            return np.tanh(z)\n",
    "        elif type == 'sigmoid':\n",
    "            return 1.0 / (1.0 + np.exp(-z))\n",
    "        elif type == 'relu':\n",
    "            return np.maximum(0, z)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def diff_actFun(self, z, type):\n",
    "        if type == 'tanh':\n",
    "            return 1.0 - (np.tanh(z))**2\n",
    "        elif type == 'sigmoid':\n",
    "            return self.actFun(z, type) * (1-self.actFun(z, type))\n",
    "        elif type == 'relu':\n",
    "            return np.where(z > 0, 1.0, 0)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def deltas_dict(self, p):\n",
    "        Z, A = self.forward_pass(p)\n",
    "        deltas = dict()\n",
    "        deltas[self.L-1] = (A[-1] - p[1])*self.diff_actFun(Z[-1], self.actFun_type)\n",
    "        for l in range(self.L-2, 0, -1):\n",
    "            deltas[l] = (self.W[l+1].T @ deltas[l+1]) *self.diff_actFun(Z[l], self.actFun_type)\n",
    "\n",
    "        return A, deltas\n",
    "\n",
    "    def stochastic_gradient_descent(self, data, alpha = 0.04, epochs = 3):\n",
    "        print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "        for k in range(epochs):\n",
    "            for p in data:\n",
    "                A, deltas = self.deltas_dict(p)\n",
    "                for i in range(1, self.L):\n",
    "                    self.W[i] = self.W[i] - alpha*deltas[i]@A[i-1].T\n",
    "                    self.B[i] = self.B[i] - alpha*deltas[i]\n",
    "        print(f\"{k} Cost = {self.MSE(data)}\")\n",
    "\n",
    "\n",
    "    def mini_batch_gradient_descent(self, data, batch_size = 15, alpha = 0.04, epochs = 3):\n",
    "        print(f\"Initial Cost = {self.MSE(data)}\")\n",
    "        data_length = len(data)\n",
    "        for k in range(epochs):\n",
    "            for j in range(0, data_length-batch_size, batch_size):\n",
    "                delta_list = []\n",
    "                A_list = []\n",
    "                for p in data[j:j+batch_size]:\n",
    "                    A, deltas = self.deltas_dict(p)\n",
    "                    delta_list.append(deltas)\n",
    "                    A_list.append(A)\n",
    "\n",
    "                for i in range(1, self.L):\n",
    "                    self.W[i] = self.W[i] - (alpha/batch_size)*sum(da[0][i]@da[1][i-1].T for da in zip(delta_list, A_list))\n",
    "                    self.B[i] = self.B[i] - (alpha/batch_size)*sum(deltas[i] for deltas in delta_list)\n",
    "            print(f\"{k} Cost = {self.MSE(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Applications on data sets <a class=\"anchor\" id=\"Applications\"></a>\n",
    "\n",
    "*Fashion MNIST* data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The *Fashion MNIST* data set is from keras. The dataset has 60,000 training data with 28x28 grayscale images and 10,000 test images. Images are labeled over 10 categories. The pixel depth allows 255 different intensities, with 0 being black and 255 being white. And the classes are: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot with labels 0 to 9 respectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the shape of train_X. We have 60000 training pictures each with 28 by 28 pixel and grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first image by putting the pixel together, and it is an Ankle boot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_X[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale images to $[0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X/255\n",
    "test_X = test_X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten images from matrix to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0].flatten().reshape(28*28, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to flatten the X matrix. Make a list first and then store the flatten vetors into it. And we also make the y to one hot encoded label vectors; we make a list and store the vectors into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for x in train_X:\n",
    "    X.append(x.flatten().reshape(28*28, 1))\n",
    "\n",
    "# Y will temp store one-hot encoded label vectors\n",
    "Y = []\n",
    "for y in train_y:\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    Y.append(temp_vec)\n",
    "\n",
    "# Our data will be stored as a list of tuples. \n",
    "train_data = [p for p in zip(X, Y)]\n",
    "\n",
    "# the same method to deal with test data\n",
    "X = []\n",
    "for x in test_X:\n",
    "  X.append(x.flatten().reshape(784, 1))\n",
    "\n",
    "Y = []\n",
    "for y in test_y:\n",
    "    temp_vec = np.zeros((10, 1))\n",
    "    temp_vec[y][0] = 1.0\n",
    "    Y.append(temp_vec)\n",
    "\n",
    "test_data = [p for p in zip(X, Y)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train MLP's using *sigmoid*, *hyperbolic tangent*, and *rectified linear* activation functions by mini batch gradient descent, and compare their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 2.7039236776001125\n",
      "0 Cost = 0.33874885370328234\n",
      "1 Cost = 0.35167746647183934\n",
      "2 Cost = 0.35498094081256715\n",
      "3 Cost = 0.30854618415549\n",
      "4 Cost = 0.32130287674188823\n"
     ]
    }
   ],
   "source": [
    "net_tanh = MultilayerPerceptron(layers=[784, 60, 60, 10], actFun_type='tanh')\n",
    "net_tanh.mini_batch_gradient_descent(train_data, batch_size = 16, alpha = 0.01, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32146002946786945"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_tanh.MSE(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 88830.85179904183\n",
      "0 Cost = 0.5\n",
      "1 Cost = 0.5\n",
      "2 Cost = 0.5\n",
      "3 Cost = 0.5\n",
      "4 Cost = 0.5\n"
     ]
    }
   ],
   "source": [
    "net_relu = MultilayerPerceptron(layers=[784, 100, 100, 10], actFun_type='relu')\n",
    "net_relu.mini_batch_gradient_descent(train_data, batch_size = 16, alpha = 0.01, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting thing here is that the cost becomes stable very fast, but it no longer decreases, staying at a relatively high state of cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_relu.MSE(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 1.4003150067878454\n",
      "0 Cost = 0.32269168107846924\n",
      "1 Cost = 0.2469432942883823\n",
      "2 Cost = 0.20577043427162678\n",
      "3 Cost = 0.18145559379543158\n",
      "4 Cost = 0.16650141660313514\n"
     ]
    }
   ],
   "source": [
    "net_sig = MultilayerPerceptron(layers=[784, 100, 100, 10], actFun_type='sigmoid')\n",
    "net_sig.mini_batch_gradient_descent(train_data, batch_size = 16, alpha = 0.01, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17143110988178312"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_sig.MSE(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing three kinds of activation functions while keeping all other hyper-parameters unchanged, *sigmoid* activation function has the best perfomance on the test data."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e918aaa81d99c652401bdd1a0c185581595fb477ac919641bd65261b5d7782a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
